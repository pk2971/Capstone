{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076425c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data of:  1813\n",
      "Reading data of:  1926\n",
      "Reading data of:  1917\n",
      "Reading data of:  1876\n",
      "Reading data of:  1992\n",
      "Reading data of:  1805\n",
      "Reading data of:  1952\n",
      "Reading data of:  1983\n",
      "Reading data of:  1817\n",
      "Reading data of:  1822\n",
      "Reading data of:  1987\n",
      "Reading data of:  1979\n",
      "Reading data of:  1872\n",
      "Reading data of:  1960\n",
      "Reading data of:  1871\n",
      "Reading data of:  1864\n",
      "Reading data of:  2000\n",
      "Reading data of:  1883\n",
      "Reading data of:  1967\n",
      "Reading data of:  1934\n",
      "Reading data of:  1849\n",
      "Reading data of:  1956\n",
      "Reading data of:  1955\n",
      "Reading data of:  1919\n",
      "Reading data of:  1855\n",
      "Reading data of:  1921\n",
      "Reading data of:  1980\n",
      "Reading data of:  1867\n",
      "Reading data of:  1995\n",
      "Reading data of:  1884\n",
      "Reading data of:  1976\n",
      "Reading data of:  1913\n",
      "Reading data of:  1937\n",
      "Reading data of:  1831\n",
      "Reading data of:  1925\n",
      "Reading data of:  1954\n",
      "Reading data of:  1944\n",
      "Reading data of:  1930\n",
      "Reading data of:  1880\n",
      "Reading data of:  1814\n",
      "Reading data of:  1965\n",
      "Reading data of:  1961\n",
      "Reading data of:  1962\n",
      "Reading data of:  2004\n",
      "Reading data of:  1828\n",
      "Reading data of:  1869\n",
      "Reading data of:  1836\n",
      "Reading data of:  1878\n",
      "Reading data of:  1910\n",
      "Reading data of:  1887\n",
      "Reading data of:  1856\n",
      "Reading data of:  1949\n",
      "Reading data of:  1998\n",
      "Reading data of:  2002\n",
      "Reading data of:  1964\n",
      "Reading data of:  1830\n",
      "Reading data of:  1970\n",
      "Reading data of:  1990\n",
      "Reading data of:  1840\n",
      "Reading data of:  1877\n",
      "Reading data of:  1912\n",
      "Reading data of:  1841\n",
      "Reading data of:  1907\n",
      "Reading data of:  1986\n",
      "Reading data of:  1811\n",
      "Reading data of:  1935\n",
      "Reading data of:  1838\n",
      "Reading data of:  1981\n",
      "Reading data of:  1958\n",
      "Reading data of:  1862\n",
      "Reading data of:  1832\n",
      "Reading data of:  1870\n",
      "Reading data of:  1890\n",
      "Reading data of:  1994\n",
      "Reading data of:  1953\n",
      "Reading data of:  1845\n",
      "Reading data of:  1866\n",
      "Reading data of:  1978\n",
      "Reading data of:  1895\n",
      "Reading data of:  1996\n",
      "Reading data of:  1977\n",
      "Reading data of:  1924\n",
      "Reading data of:  1812\n",
      "Reading data of:  1894\n",
      "Reading data of:  1892\n",
      "Reading data of:  1982\n",
      "Reading data of:  1975\n",
      "Reading data of:  1938\n",
      "Reading data of:  1835\n",
      "Reading data of:  1873\n",
      "Reading data of:  1942\n",
      "Reading data of:  1984\n",
      "Reading data of:  1945\n",
      "Reading data of:  1923\n",
      "Reading data of:  1888\n",
      "Reading data of:  1807\n",
      "Reading data of:  1861\n",
      "Reading data of:  1846\n",
      "Reading data of:  1968\n",
      "Reading data of:  1885\n",
      "Reading data of:  1893\n",
      "Reading data of:  1918\n",
      "Reading data of:  1999\n",
      "Reading data of:  1973\n",
      "Reading data of:  1936\n",
      "Reading data of:  2001\n",
      "Reading data of:  1957\n",
      "Reading data of:  1904\n",
      "Reading data of:  1993\n",
      "Reading data of:  1886\n",
      "Reading data of:  1881\n",
      "Reading data of:  1854\n",
      "Reading data of:  1914\n",
      "Reading data of:  1899\n",
      "Reading data of:  1966\n",
      "Reading data of:  1920\n",
      "Reading data of:  1946\n",
      "Reading data of:  1859\n",
      "Reading data of:  1850\n",
      "Reading data of:  1927\n",
      "Reading data of:  1857\n",
      "Reading data of:  1897\n",
      "Reading data of:  1951\n",
      "Reading data of:  1842\n",
      "Reading data of:  1972\n",
      "Reading data of:  1900\n",
      "Reading data of:  1971\n",
      "Reading data of:  1959\n",
      "Reading data of:  1963\n",
      "Reading data of:  1988\n",
      "Reading data of:  1825\n",
      "Reading data of:  1860\n",
      "Reading data of:  1847\n",
      "Reading data of:  1947\n",
      "Reading data of:  1902\n",
      "Reading data of:  1905\n",
      "Reading data of:  1939\n",
      "Reading data of:  1806\n",
      "Reading data of:  1940\n",
      "Reading data of:  1928\n",
      "Reading data of:  1898\n",
      "Reading data of:  1843\n",
      "Reading data of:  1804\n",
      "Reading data of:  1974\n",
      "Reading data of:  1989\n",
      "Reading data of:  1875\n",
      "Reading data of:  1889\n",
      "Reading data of:  1874\n",
      "Reading data of:  1868\n",
      "Reading data of:  1991\n",
      "Reading data of:  1809\n",
      "Reading data of:  1950\n",
      "Reading data of:  1909\n",
      "Reading data of:  1837\n",
      "Reading data of:  1969\n",
      "Reading data of:  1852\n",
      "Reading data of:  1851\n",
      "Reading data of:  1844\n",
      "Reading data of:  1863\n",
      "Reading data of:  2003\n",
      "Reading data of:  1896\n",
      "Reading data of:  1922\n",
      "Reading data of:  1985\n",
      "Reading data of:  2005\n",
      "Reading data of:  1833\n",
      "Reading data of:  1906\n",
      "Reading data of:  1821\n",
      "Reading data of:  1943\n",
      "Reading data of:  1997\n",
      "Reading data of:  1834\n",
      "Reading data of:  1826\n",
      "Reading data of:  1948\n",
      "Reading data of:  1901\n",
      "Reading data of:  1858\n",
      "Reading data of:  1941\n",
      "Reading data of:  1815\n",
      "Reading data of:  1853\n",
      "Reading data of:  1915\n",
      "Reading data of:  1823\n",
      "Reading data of:  1865\n",
      "Reading data of:  1848\n",
      "Reading data of:  1929\n",
      "Reading data of:  1931\n",
      "Reading data of:  1903\n",
      "Reading data of:  1932\n",
      "Reading data of:  1819\n",
      "Reading data of:  1808\n",
      "Reading data of:  1908\n",
      "Reading data of:  1933\n",
      "Reading data of:  1839\n",
      "Reading data of:  1824\n",
      "Reading data of:  1820\n",
      "Reading data of:  1911\n",
      "Reading data of:  1818\n",
      "Reading data of:  1879\n",
      "Reading data of:  1891\n",
      "Reading data of:  1916\n",
      "Reading data of:  1882\n",
      "Reading data of:  1803\n",
      "Reading data of:  1827\n",
      "Reading data of:  1810\n",
      "Reading data of:  1013\n",
      "Reading data of:  1101\n",
      "Reading data of:  2006\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the text files\n",
    "input_directory = \"/Users/praharshita/Desktop/Capstone/textfiles\"\n",
    "\n",
    "# Output directory where merged files will be saved\n",
    "output_directory = \"/Users/praharshita/Desktop/Capstone/merged_files_by_year\"\n",
    "\n",
    "# List all files in the input directory\n",
    "files = os.listdir(input_directory)\n",
    "\n",
    "# Create a dictionary to organize files by year\n",
    "yearly_files = {}\n",
    "\n",
    "# Iterate through each file and group them by year\n",
    "for file in files:\n",
    "    if '-' in file:\n",
    "        year = file.split('-')[0]\n",
    "        if year not in yearly_files:\n",
    "            yearly_files[year] = []\n",
    "#print(yearly_files)\n",
    "all_files = os.listdir(input_directory)\n",
    "\n",
    "for year, files_in_year in yearly_files.items():\n",
    "    print(\"Reading data of: \",year)\n",
    "    filtered_files = [file for file in all_files if file.startswith(year)]\n",
    "    for s in filtered_files:\n",
    "        yearly_files[year].append(s)\n",
    "    merged_text = \"\"\n",
    "    for file in files_in_year:\n",
    "        with open(os.path.join(input_directory, file), 'r', encoding='utf-8') as f:\n",
    "            merged_text += f.read()\n",
    "    output_filename = os.path.join(output_directory, f\"{year}.txt\")\n",
    "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(merged_text)\n",
    "    #print(files_in_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba1728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "# Lower case\n",
    "# Tokenize\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string  # Import the string module for punctuation removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3866b2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/praharshita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning file: 1  Year: 1912.txt\n",
      "Cleaning file: 2  Year: 1906.txt\n",
      "Cleaning file: 3  Year: 2002.txt\n",
      "Cleaning file: 4  Year: 1899.txt\n",
      "Cleaning file: 5  Year: 1866.txt\n",
      "Cleaning file: 6  Year: 1872.txt\n",
      "Cleaning file: 7  Year: 1873.txt\n",
      "Cleaning file: 8  Year: 1867.txt\n",
      "Cleaning file: 9  Year: 1898.txt\n",
      "Cleaning file: 10  Year: 2003.txt\n",
      "Cleaning file: 11  Year: 1907.txt\n",
      "Cleaning file: 12  Year: 1913.txt\n",
      "Cleaning file: 13  Year: 1939.txt\n",
      "Cleaning file: 14  Year: 1905.txt\n",
      "Cleaning file: 15  Year: 1911.txt\n",
      "Cleaning file: 16  Year: 2001.txt\n",
      "Cleaning file: 17  Year: 1859.txt\n",
      "Cleaning file: 18  Year: 1871.txt\n",
      "Cleaning file: 19  Year: 1865.txt\n",
      "Cleaning file: 20  Year: 1864.txt\n",
      "Cleaning file: 21  Year: 1870.txt\n",
      "Cleaning file: 22  Year: 1858.txt\n",
      "Cleaning file: 23  Year: 2000.txt\n",
      "Cleaning file: 24  Year: 1910.txt\n",
      "Cleaning file: 25  Year: 1904.txt\n",
      "Cleaning file: 26  Year: 1938.txt\n",
      "Cleaning file: 27  Year: 1900.txt\n",
      "Cleaning file: 28  Year: 1914.txt\n",
      "Cleaning file: 29  Year: 1928.txt\n",
      "Cleaning file: 30  Year: 2004.txt\n",
      "Cleaning file: 31  Year: .DS_Store\n",
      "UnicodeDecodeError: Skipping file '.DS_Store' due to encoding issue.\n",
      "Cleaning file: 32  Year: 1874.txt\n",
      "Cleaning file: 33  Year: 1860.txt\n",
      "Cleaning file: 34  Year: 1848.txt\n",
      "Cleaning file: 35  Year: 1849.txt\n",
      "Cleaning file: 36  Year: 1861.txt\n",
      "Cleaning file: 37  Year: 1875.txt\n",
      "Cleaning file: 38  Year: 2005.txt\n",
      "Cleaning file: 39  Year: 1929.txt\n",
      "Cleaning file: 40  Year: 1915.txt\n",
      "Cleaning file: 41  Year: 1901.txt\n",
      "Cleaning file: 42  Year: 1917.txt\n",
      "Cleaning file: 43  Year: 1903.txt\n",
      "Cleaning file: 44  Year: 1888.txt\n",
      "Cleaning file: 45  Year: 1863.txt\n",
      "Cleaning file: 46  Year: 1877.txt\n",
      "Cleaning file: 47  Year: 1876.txt\n",
      "Cleaning file: 48  Year: 1862.txt\n",
      "Cleaning file: 49  Year: 1889.txt\n",
      "Cleaning file: 50  Year: 2006.txt\n",
      "Cleaning file: 51  Year: 1902.txt\n",
      "Cleaning file: 52  Year: 1916.txt\n",
      "Cleaning file: 53  Year: 1959.txt\n",
      "Cleaning file: 54  Year: 1971.txt\n",
      "Cleaning file: 55  Year: 1965.txt\n",
      "Cleaning file: 56  Year: 1839.txt\n",
      "Cleaning file: 57  Year: 1805.txt\n",
      "Cleaning file: 58  Year: 1811.txt\n",
      "Cleaning file: 59  Year: 1810.txt\n",
      "Cleaning file: 60  Year: 1804.txt\n",
      "Cleaning file: 61  Year: 1838.txt\n",
      "Cleaning file: 62  Year: 1964.txt\n",
      "Cleaning file: 63  Year: 1970.txt\n",
      "Cleaning file: 64  Year: 1958.txt\n",
      "Cleaning file: 65  Year: 1999.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\u2014\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Tokenize the text into words\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Remove stop words and punctuation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/tokenize/destructive.py:178\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    175\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 178\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the path to the input directory\n",
    "file_path = \"/Users/praharshita/Desktop/Capstone/merged_files_by_year\"\n",
    "\n",
    "# Set the path to the output directory\n",
    "output_directory = \"/Users/praharshita/Desktop/Capstone/cleaned_files_by_year\"\n",
    "n=1\n",
    "# Initialize NLTK's stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# List all files in the input directory\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "for file in files:\n",
    "    print(\"Cleaning file:\", n,\" Year:\",str(file))\n",
    "    n+=1\n",
    "    # Read data from the file with a more permissive encoding\n",
    "    with open(os.path.join(file_path, file), \"r\", encoding=\"utf-8-sig\") as input_file:\n",
    "        try:\n",
    "            text = input_file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UnicodeDecodeError: Skipping file '{file}' due to encoding issue.\")\n",
    "            continue\n",
    "\n",
    "    text = text.replace(u'\\u2014', ' ')\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Reconstruct the text without stop words and punctuation\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "    # Extract the file name without extension\n",
    "    base_file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "    # Construct the output file path\n",
    "    output_filename = os.path.join(output_directory, f\"{base_file_name}.txt\")\n",
    "\n",
    "    # Overwrite the data into the same file\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(filtered_text)\n",
    "\n",
    "print(\"Stop words and punctuation removed, and data overwritten successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
